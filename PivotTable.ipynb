{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from operator import add\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql import SQLContext\n",
    "from operator import add\n",
    "import numpy as np\n",
    "import string\n",
    "sc = pyspark.SparkContext('local[*]',appName=\"DocClassification\")\n",
    "sqlc = SQLContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to create some really simple dummy documents as a proof of concept. Each document is a string, like we would deal with. We'll map them like we usually do, with a slight twist. After we zip with index (to get a document id) and swap them into (did,doc) shape, then map them into (did,wid) pairs and lowercase them, we're going to map them to (did,wid,1) tuples, instead of the usual ((did,wid),1) pairs where we can then reduceByKey(add) to get ((did,wid),count) values. The former is a good representation for a sparse matrix, but the latter is going to let us create a dataframe more easilly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'this', 1), (0, 'is', 1), (0, 'something', 1), (0, 'i', 1), (0, 'really', 1), (0, 'want', 1), (0, 'to', 1), (0, 'test', 1), (1, 'to', 1), (1, 'test', 1), (1, 'something', 1), (1, 'is', 1), (1, 'something', 1), (1, 'i', 1), (1, 'really', 1), (1, 'want', 1), (1, 'to', 1), (1, 'do', 1), (2, 'i', 1), (2, 'need', 1), (2, 'to', 1), (2, 'do', 1), (2, 'something', 1), (2, 'that', 1), (2, 'i', 1), (2, 'want', 1), (2, 'to', 1), (2, 'do', 1), (2, 'which', 1), (2, 'is', 1), (2, 'to', 1), (2, 'test', 1), (2, 'something', 1)]\n"
     ]
    }
   ],
   "source": [
    "def swap(x):\n",
    "    return (x[1],x[0])\n",
    "\n",
    "strs = sc.parallelize([\"This is something I really want to test.\",\n",
    "                       \"To test something is something I really want to do.\",\n",
    "                       \"I need to do something that I want to do which is to test something.\"])\n",
    "strs = strs.zipWithIndex()\\\n",
    "            .map(lambda x: swap(x))\\\n",
    "            .flatMapValues(lambda x: x.split())\\\n",
    "            .mapValues(lambda x: x.lower().strip(string.punctuation))\\\n",
    "            .map(lambda x: (x[0],x[1],1))\n",
    "\n",
    "print(strs.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll throw them into a dataframe. It is of course not in the correct order as you can see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "|did|      wid|count|\n",
      "+---+---------+-----+\n",
      "|  0|     this|    1|\n",
      "|  0|       is|    1|\n",
      "|  0|something|    1|\n",
      "|  0|        i|    1|\n",
      "|  0|   really|    1|\n",
      "|  0|     want|    1|\n",
      "|  0|       to|    1|\n",
      "|  0|     test|    1|\n",
      "|  1|       to|    1|\n",
      "|  1|     test|    1|\n",
      "|  1|something|    1|\n",
      "|  1|       is|    1|\n",
      "|  1|something|    1|\n",
      "|  1|        i|    1|\n",
      "|  1|   really|    1|\n",
      "|  1|     want|    1|\n",
      "|  1|       to|    1|\n",
      "|  1|       do|    1|\n",
      "|  2|        i|    1|\n",
      "|  2|     need|    1|\n",
      "+---+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlc.createDataFrame(strs,schema = ['did','wid','count'])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the magic happens. If we groupBy('did') then we can pivot('wid'). Basically this turns each distinct value in the 'wid' column into its own column. We can really only work with this though if we use some sort of aggregation function. That's where the sum('count') comes in, and why we kept it in this strange format until now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+---+----+------+---------+----+----+----+---+----+-----+\n",
      "|did|  do|  i| is|need|really|something|test|that|this| to|want|which|\n",
      "+---+----+---+---+----+------+---------+----+----+----+---+----+-----+\n",
      "|  0|null|  1|  1|null|     1|        1|   1|null|   1|  1|   1| null|\n",
      "|  1|   1|  1|  1|null|     1|        2|   1|null|null|  2|   1| null|\n",
      "|  2|   2|  2|  1|   1|  null|        2|   1|   1|null|  3|   1|    1|\n",
      "+---+----+---+---+----+------+---------+----+----+----+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.groupBy('did').pivot('wid').sum('count')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just turn the null values into 0s and we're on our way home..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+------+---------+----+----+----+---+----+-----+\n",
      "|did| do|  i| is|need|really|something|test|that|this| to|want|which|\n",
      "+---+---+---+---+----+------+---------+----+----+----+---+----+-----+\n",
      "|  0|  0|  1|  1|   0|     1|        1|   1|   0|   1|  1|   1|    0|\n",
      "|  1|  1|  1|  1|   0|     1|        2|   1|   0|   0|  2|   1|    0|\n",
      "|  2|  2|  2|  1|   1|     0|        2|   1|   1|   0|  3|   1|    1|\n",
      "+---+---+---+---+----+------+---------+----+----+----+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.na.fill(0)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
